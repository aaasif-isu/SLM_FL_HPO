
experiment:
  seed: 2025

model:
  use_lora: true              # true = enable LoRA; false = run frozen base (no adapter training)
  adapter_mode: per_cluster   # per_cluster | single
  lora:
    r: 4
    alpha: 16
    dropout: 0.05
    step_lr: 5e-5
    max_grad_norm: 1.0
    kl_max: 0.05
    every_k_rounds: 3         # LoRA update frequency (policy_adapter.policy_update gate)

stability:

  reward:
    lambda_penalty: 0.3   # weight on instability in reward = Δacc - λ * instability
    scale: 1.0            # optional: multiply reward before sending to LoRA

  # SUGGEST pre-gate (happens BEFORE any HP agent LLM call)
  pre_gate:
    min_round_gap: 0          # minimum rounds between *successful* HP updates for a client
    min_delta: 0.002            # require |Δacc| ≥ this to call HP agent (e.g., 0.002)
    require_lyapunov: true   # if true, Lyapunov must pass to allow HP call

  # Lyapunov parameters (used by workflow._lyapunov_pass)
  lyapunov:
    #Higher beta (0.5–1.0) penalizes jitter more → harder to pass (fewer updates).
    #Lower beta (0.1–0.3) is more forgiving to noise → easier to pass.
    beta: 0.3
    # Epsilon controls sensitivity to small accuracy changes.
    # Smaller eps (0.001–0.005) makes Lyapunov more sensitive to small changes.
    # Larger eps (0.01–0.05) makes it less sensitive.
    base_eps: 0.005
    # How many recent training losses to consider for EMA and variance
    #Larger window (6–10) smooths more, reacts slower but more stable.
    #Smaller window (3–4) reacts faster but is noisier.
    window: 2

agents:
  slm_model: "Qwen/Qwen2.5-0.5B-Instruct"


# ============================================
# Model and Dataset Configuration
# ============================================

# The neural network architecture to be used.
# Options:
#   - ResNet18  : For image classification tasks (CIFAR-10, MNIST, PACS, OfficeHome)
#   - bert      : For NLP tasks (Shakespeare, text datasets)
#   - charlstm  : Character-level LSTM for text-based datasets (Shakespeare)
model_name: ResNet18

# The dataset to be used in the experiment.
# Options:
#   - cifar10
#   - mnist
#   - femnist
#   - shakespeare
#   - pacs
#   - officehome
dataset_name: cifar10

# Controls class imbalance in the dataset.
# 1.0 means balanced dataset.
# Values < 1.0 introduce imbalance; e.g., 0.5 means half the samples for some classes.
imbalance_ratio: 1.0

# Fractions of training and testing data to use.
# 1.0 means using the full dataset.
train_sample_fraction: 0.1
test_sample_fraction: 0.1

# Minimum number of samples each client must have.
# Prevents clients from having too few data points in federated settings.
min_samples_per_client: 30

# Number of client clusters to group based on resource profile or data distribution.
# Used in clustered federated learning or personalized FL.
num_clusters: 3


# ============================================
# Federated Learning Configuration
# ============================================

# Total number of participating clients in the federation.
# Typical values:
#   - CIFAR-10/MNIST : 500
#   - FEMNIST        : 3550 (use fractions for scaling, e.g., 0.003)
#   - Shakespeare    : 1129
#   - PACS           : 4
num_clients: 3

# Number of global communication rounds (epochs).
# Each round consists of multiple local updates and aggregation.
global_epochs: 20

# Fraction of clients selected per round for training.
# E.g., 0.3 means 30% of total clients participate each round.
frac: 1

# Federated learning mode.
# Options:
#   - splitfed    : Split Federated Learning (model split between client/server)
#   - centralized : Traditional centralized training (single model)
fl_mode: splitfed


# ============================================
# Training Control Parameters
# ============================================

training_params:
  # Early stopping patience for local training.
  # Training stops if validation accuracy does not improve for 'patience' epochs.
  patience: 20

  # Minimum change in validation accuracy to be considered an improvement.
  min_delta: 0.01


# ============================================
# Hyperparameter Optimization (HPO) Agent Configuration
# ============================================

hpo_strategy:
  # The hyperparameter optimization method to use.
  # Options:
  #   - agent         : LLM-based HPO agent (dynamic and adaptive)
  #   - random_search : Randomized hyperparameter search
  #   - sha           : Successive Halving Algorithm
  #   - bo            : Bayesian Optimization
  #   - fixed         : Fixed hyperparameters (no optimization)
  method: agent

  # Path where the HPO agent saves and loads client-specific HPO state.
  hpo_checkpoint_path: agent/client_hpo_states.yaml

  # Historical window size (number of past rounds) to use for trend analysis.
  # 0 disables historical context and uses only current round data.
  history_window: 5

  # Early stopping for HPO at the client level.
  # If a client's accuracy hasn't improved for 'hpo_patience' rounds,
  # the agent stops optimizing that client to save computation.
  hpo_patience: 3

# Logging json file to record HPO events and decisions.
logging:
  json_dir: "/u/aalasif/SLM_FL_HPO/code/agent/json_files"
